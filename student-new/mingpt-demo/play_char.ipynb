{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 773198 characters, 4396 unique.\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "with open('input.txt', 'rb') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "encoding_info = chardet.detect(raw_data)\n",
    "detected_encoding = encoding_info['encoding']\n",
    "\n",
    "text = open('input.txt', 'r', encoding='gb18030').read()\n",
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "#text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/08/2025 15:21:06 - INFO - mingpt.model -   number of parameters: 4.239667e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=12, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1510 [00:00<?, ?it/s]/home/yuzhong/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 iter 1509: train loss 0.36099. lr 3.000244e-04: 100%|██████████| 1510/1510 [20:12<00:00,  1.25it/s]\n",
      "epoch 2 iter 1509: train loss 0.19898. lr 6.000000e-05: 100%|██████████| 1510/1510 [20:16<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们都是憋老仔，脖子上喜欢挂玉佩。来财， 来， 来旺儿挑担儿受私贿\n",
      "\n",
      "    \n",
      "　　诗曰：\n",
      "　　簟展湘纹浪欲生，幽怀自感梦难成。\n",
      "　　倚床剩觉添风味，开户羞将待月明。\n",
      "　　拟倩蜂媒传密意，难将萤火照离情。\n",
      "　　遥怜织女佳期近，时看银河几曲横。\n",
      "　　话说一日，陈敬济听见薛嫂儿说知孙雪娥之事。这陈敬济乘着这个根由，就如此这般，使薛嫂儿往西门庆家对月娘说。薛嫂只得见月娘，说：“陈姑夫在外声言发话，说不要大姐，要写状子，巡抚、巡按处告示，说老爹在日，收着他父亲寄放的许多金银箱笼细软之物。”这月娘一来因孙雪娥被来旺儿盗财拐去，二者又是来安儿小厮走了，三者家人来兴媳妇惠秀又死了，刚打发出去，家中正七事八事，听见薛嫂儿来说此话，唬的慌了手脚，连忙雇轿子，打发大姐家去。但是大姐床奁箱厨陪嫁之物，交玳安雇人，都抬送到陈敬济家。敬济说：“这是他随身嫁我的床帐妆奁，还有我家寄放的细软金银箱笼，须索还我。”薛嫂道：“你大丈母说来，当初丈人在时，止收下这个床奁嫁妆，并没见你别的箱笼。”敬济又要使女元宵儿。薛嫂儿和玳安儿来对月娘说。月娘不肯把元宵与他，说：“这丫头是李娇儿房中使的，如今留着晚早看哥儿哩。”把中秋儿打发将来，说：“原是买了伏侍大姐的。”这敬济又不要中秋儿，两头来回只教薛嫂儿走。他娘张氏向玳安说：“哥哥，你到家拜上你大娘，你家姐儿们多，也不稀罕这个使女看守哥儿。既是与了大姐房里好一向，你姐夫已是收用过了他，你大娘只顾留怎的？”玳安一面到家，把此话对月娘说了。月娘无言可对，只得把元宵儿打发将来。敬济收下，满心欢喜，说道：“可怎的也打我这条道儿来？”正是：\n",
      "　　饶你奸似鬼，吃我洗脚水。\n",
      "　　按下一头。单说李知县儿子李衙内，自从清明郊外看见吴月娘、孟玉楼两人一般打扮，生的俱有姿色，知是西门庆妻小。衙内有心，爱孟玉楼生的长挑身材，瓜子面皮，模样儿风流俏丽。原来衙内丧偶，鳏居已久，一向着媒妇各处求亲，都不遂意。及见玉楼，便觉动心，但无门可入，未知嫁与不嫁，从违如何。不期雪娥缘事在官，已知是西门庆家出来的，周旋委曲，在伊父案前，将各犯用刑研审，追出赃物数目，望其来领。月娘害怕，又不使人见官。衙内失望，因此才将赃物入官，雪娥官卖。至是衙内谋之于廊吏何不韦，径使官媒婆陶妈妈来西门庆家访求亲事，许说成此门亲事，免县中打卯，还赏银五两。\n",
      "　　这陶妈妈听了，喜欢的疾走如飞，一日到于西门庆门首。来昭正在门首立，只见陶妈妈向前道了万福，说道：“动问管家哥一声，此是西门老爹家？”来昭道：“你是那里来的？老爹已下世了，有甚话说？”陶妈妈道：“累及管家进去禀声，我是本县官媒人，名唤陶妈妈，奉衙内小老爹钧语，分付说咱宅内有位奶奶要嫁人，敬来说亲。”那来昭喝道：“你这婆子，好不近理！我家老爹没了一年有余，止有两位奶奶守寡，并不嫁人。常言疾风暴雨，不入寡妇之门。你这媒婆，有要没紧，走来胡撞甚亲事？还不走快着，惹的后边奶奶知道，一顿好打。”那陶妈妈笑道：“管家哥，常言官差吏差，来人不差。小老爹不使我，我敢来？嫁不嫁，起动进去禀声，我好回话去。”来昭道：“也罢，与人方便，自己方便，你少待片时，等我进去。两位奶奶，一位奶奶有哥儿，一位奶奶无哥儿，不知是那一位奶奶要嫁人？”陶妈妈道：“衙内小老爹说，清明那日郊外曾看见来，是面上有几点白麻子的那位奶奶。”\n",
      "　　来昭听了，走到后边，如此这般告诉月娘说：“县中使了个官媒人在外面。”倒把月娘吃了一惊，说：“我家并没半个字儿迸出，外边人怎得晓的？”来昭道：“曾在郊外，清明那日见来，说脸上有几个白麻子儿的。”月娘便道：“莫不孟三姐也‘腊月里罗卜－－动人心’？忽剌八要往前进嫁人？正是‘世间海水知深浅，惟有人心难忖量’”。一面走到玉楼房中坐下，便问：“孟三娘，奴有件事儿来问你，外面有个保山媒人，说是县中小衙内，清明那日曾见你一面，说你要往前进。端的有此话么？”看官听说，当时没巧不成话，自古姻缘着线牵。那日郊外，孟玉楼看见衙内生的一表人物，风流博浪，两家年甲多相仿佛，又会走马拈弓弄箭，彼此两情四目都有意，已在不言之表。但未知有妻子无妻子，口中不言，心内暗度：“男子汉已死，奴身边又无所出。虽故大娘有孩儿，到明日长大了，各肉儿各疼。闪的我树倒无阴，竹篮儿打水。”又见月娘自有了孝哥儿，心肠改变，不似往时，“我不如往前进一步，寻上个叶落归根之处，还只顾傻傻的守些甚么？到没的担阁了奴的青春年少。”正在思慕之间，不想月娘进来说此话，正是清明郊外看见的那个人，心中又是欢喜，又是羞愧，口里虽说：“大娘休听人胡说，奴并没此话。”不觉把脸来飞红了，正是：\n",
      "　　含羞对众休开口，理鬓无言只揾头。\n",
      "　　月娘说：“此是各人心里事，奴也管不的许多。”一面叫来昭：“你请那保山进来。”来昭门首唤陶妈妈，进到后边见月娘，行毕了礼数，坐下。小丫鬟倒茶吃了。月娘便问：“保山来，有甚事？”陶妈妈便道：“小媳妇无事不登三宝殿，奉本县正宅衙内分付，说贵宅上有一位奶奶要嫁人，讲说亲事。”月娘道：“俺家这位娘子嫁人，又没曾传出去，你家衙内怎得知道？”陶妈妈道：“俺家衙内说来，清明那日，在郊外亲见这位娘子，生的长挑身材，瓜子面皮，脸上有稀稀几个白麻子，便是这位奶奶。”月娘听了，不消说就是孟三姐了。于是领陶妈妈到玉楼房中明间内坐下。\n",
      "　　等勾多时，玉楼梳洗打扮出来。陶妈妈道了万福，说道：“就是此位奶奶，果然话不虚传，人材出众，盖世无双，堪可与俺衙内老爹做个正头娘子。”玉楼笑道：“妈妈休得乱说。且说你衙内今年多大年纪？原娶过妻小没有？房中有人也无？姓甚名谁？有官身无官身无官身？从实说来，休要捣谎。”陶妈妈道：“天么，天么！小媳妇是本县官媒，不比外边媒人快说谎。我有一句说一句，并无虚假。俺知县老爹年五十多岁，止生了衙内老爹一人，今年属马的，三十一岁，正月二十三日辰时建生。见做国子监上舍，不久就是举人、进士。有满腹文章，弓马熟闲，诸子百家，无不通晓。没有大娘子二年光景，房内止有一个从嫁使女答应，又不出众。要寻个娘子当家，敬来宅上说此亲事。若是咱府上做这门亲事，老爹说来，门面差摇，坟茔地土钱粮，一例尽行蠲免，有人欺负，指名说来，拿到县里，任意拶打。”玉楼道：“你衙内有儿女没有？原籍那里人氏？诚恐一时任满，千山万水带去，奴亲都在此处，莫不也要同他去？”陶妈妈道：“俺衙内身边，儿花女花没有，好不单径。原籍是咱北京真定府枣强县人氏，过了黄河不上六七百里。他家中田连阡陌，骡马成群，人丁无数，走马牌楼，都是抚按明文，圣旨在上，好不赫耀吓人。如今娶娘子到家，做了正房，过后他得了官，娘子便是五花官诰，坐七香车，为命妇夫人，有何不好？”这孟玉楼被陶妈妈一席话，说得千肯万肯，一面唤兰香放桌儿，看茶食点心与保山吃。因说：“保山，你休怪我叮咛盘问。你这媒人们说谎的极多，奴也吃人哄怕了。”陶妈妈道：“好奶奶，只要一个比一个。清自清，浑自浑，好的带累了歹的。小媳妇并不捣谎，只依本分做媒。奶奶若肯了，写个婚帖儿与我，好回小老爹话去。”玉楼取了一条大红段子，使玳安交铺子里傅伙计写了生时八字。吴月娘便说：“你当初原是薛嫂儿说的媒，如今还使小厮叫将薛嫂儿来，两个同拿了贴儿去，说此亲事，才是礼。”不多时，使玳安儿叫了薛嫂儿来，见陶妈妈道了万福。当行见当行，拿着贴儿出离西门庆家门，往县中回衙内话去。一个是这里冰人，一个是那头保山，两张口四十八个牙，这一去管取说得月\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "#context = \"My God !, O God! you can't do this thing!\"\n",
    "context = '我们都是憋老仔，脖子上喜欢挂玉佩。来财， 来， 来'\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 3000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
